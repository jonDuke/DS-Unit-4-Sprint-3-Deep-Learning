{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13NcYoBTmhw",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAieTO2Ya0BD",
        "colab_type": "text"
      },
      "source": [
        "### Load and clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "import requests\n",
        "\n",
        "r = requests.get('https://www.gutenberg.org/files/100/100-0.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ejq2k-p9f7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7e6e912b-9938-4439-da23-2f17577b5541"
      },
      "source": [
        "text = r.content.decode('utf-8')\n",
        "text[:500]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff\\r\\nProject Gutenberg’s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not located in the United States, you’ll\\r\\nhave to check the laws of the country where '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xyQvVXh-EZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8aec3bdc-b799-48f7-ba3e-76c29ebd36ea"
      },
      "source": [
        "# remove some of the extra characters\n",
        "text = text.replace('\\r', '')\n",
        "text[:500]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff\\nProject Gutenberg’s The Complete Works of William Shakespeare, by William\\nShakespeare\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org.  If you are not located in the United States, you’ll\\nhave to check the laws of the country where you are l'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNSIn1CV-uhp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "59b86670-7699-42bb-e155-833619b3929a"
      },
      "source": [
        "# remove the preface so we have just Shakespear's works (keeping table of contents for now)\n",
        "text = text[827:]\n",
        "text[:500]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Complete Works of William Shakespeare\\n\\n\\n\\nby William Shakespeare\\n\\n\\n\\n\\n      Contents\\n\\n\\n\\n               THE SONNETS\\n\\n               ALL’S WELL THAT ENDS WELL\\n\\n               THE TRAGEDY OF ANTONY AND CLEOPATRA\\n\\n               AS YOU LIKE IT\\n\\n               THE COMEDY OF ERRORS\\n\\n               THE TRAGEDY OF CORIOLANUS\\n\\n               CYMBELINE\\n\\n               THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n               THE FIRST PART OF KING HENRY THE FOURTH\\n\\n               THE SECOND PART OF KING '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwS-6rwo93Fp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab7e021d-47f3-44bc-d5f2-9d22bcb676b1"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5572325"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCf8nidfa43I",
        "colab_type": "text"
      },
      "source": [
        "### Encode the data as sequences of characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voRtiTqxALpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDcrKiIR_6yI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b7f8db5-b262-4f64-e6c6-63702b8116ab"
      },
      "source": [
        "# break the text into character sequences\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is maxlen chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  1114457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZ9prW5AYQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8cd617ce-438c-4263-b8d4-2328ff76a6e2"
      },
      "source": [
        "# Create X & y\n",
        "import numpy as np\n",
        "\n",
        "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        X[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1114457, 40, 105)\n",
            "(1114457, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y_K5Q4HbAKs",
        "colab_type": "text"
      },
      "source": [
        "### Define functions for previewing predictions during the training\n",
        "\n",
        "From the lecture notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3aREr9eBnLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg4X0I1xBpDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "import random\n",
        "import sys\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    # Only print a preview every 5 epochs\n",
        "    if (epoch % 5 == 0):\n",
        "      print()\n",
        "      print('----- Generating text after Epoch: %d' % epoch)\n",
        "      \n",
        "      start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "      \n",
        "      generated = ''\n",
        "      \n",
        "      sentence = text[start_index: start_index + maxlen]\n",
        "      generated += sentence\n",
        "      \n",
        "      print('----- Generating with seed: \"' + sentence + '\"')\n",
        "      sys.stdout.write(generated)\n",
        "      \n",
        "      for i in range(400):\n",
        "          x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "          for t, char in enumerate(sentence):\n",
        "              x_pred[0, t, char_int[char]] = 1\n",
        "              \n",
        "          preds = model.predict(x_pred, verbose=0)[0]\n",
        "          next_index = sample(preds)\n",
        "          next_char = int_char[next_index]\n",
        "          \n",
        "          sentence = sentence[1:] + next_char\n",
        "          \n",
        "          sys.stdout.write(next_char)\n",
        "          sys.stdout.flush()\n",
        "      print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPyQIog-bH8x",
        "colab_type": "text"
      },
      "source": [
        "### Build and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwATCgC5BYUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the LSTM model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWm8Ca9By27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "023c7795-fd8d-4b29-bf45-317703e9c911"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y,\n",
        "          batch_size=32,\n",
        "          epochs=50,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 2.0075\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"d we create, in absence of ourself,\n",
            "    \"\n",
            "d we create, in absence of ourself,\n",
            "    These bett!\n",
            "  KINE. There some, the Pisius\n",
            "    Ale untidcest givenn that it swarniar,\n",
            "    None.\n",
            "\n",
            "ALMICO.\n",
            "      Lay, I how discher's, thish dimenor my buss\n",
            "    And surd anflesh, not lood than her denty,\n",
            "Th’e wet not delisg toomancy crick a bul\n",
            "    Theirath affeny wasted the Casbant,\n",
            "    Lexter couss as dy coblougs the tongcted.\n",
            "\n",
            "THUST. Yis come sucp hem blessioned raice\n",
            "the dight.\n",
            "\n",
            "PARIOP.\n",
            "What hav\n",
            "34827/34827 [==============================] - 196s 6ms/step - loss: 2.0074\n",
            "Epoch 2/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.7040\n",
            "Epoch 3/50\n",
            "34827/34827 [==============================] - 184s 5ms/step - loss: 1.6050\n",
            "Epoch 4/50\n",
            "34827/34827 [==============================] - 183s 5ms/step - loss: 1.5509\n",
            "Epoch 5/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.5158\n",
            "Epoch 6/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 1.4913\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"y Council,\n",
            "    As more at large your Gra\"\n",
            "y Council,\n",
            "    As more at large your Grame with yellows it,\n",
            "Nock more by messen conjudge rateer\n",
            "would my instrome.\n",
            "\n",
            "ORBRICONT.\n",
            "What do you not sair, did we ferst dless’d,\n",
            "    chose makew to boft the man in Senn a diver\n",
            "Hurtel coits over Boingly Cousied for you.\n",
            "  SPBETESS. What, good fire of your bearth and acquay.\n",
            "\n",
            "Hathing with astressing and words in\n",
            "What I corvelt, it be thee would I will inly\n",
            "To please of the gance this, no morish’d\n",
            "34827/34827 [==============================] - 196s 6ms/step - loss: 1.4913\n",
            "Epoch 7/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.4727\n",
            "Epoch 8/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.4575\n",
            "Epoch 9/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.4454\n",
            "Epoch 10/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.4352\n",
            "Epoch 11/50\n",
            "34824/34827 [============================>.] - ETA: 0s - loss: 1.4263\n",
            "----- Generating text after Epoch: 10\n",
            "----- Generating with seed: \"not change my horse with any that\n",
            "treads\"\n",
            "not change my horse with any that\n",
            "treadst so music, with it paisons my devine!\n",
            "We to trueth, and to brooketszunest.\n",
            "\n",
            "LEAR.\n",
            "I’ll be, you are sicks son with an Exbage\n",
            "Bound. Alas, that Look Pleneant, the Tram of Francis,\n",
            "    Beside hands, is sleep, in hard as whore new,\n",
            "With recutest of alary aget by yours. In the lows you\n",
            "Doth glorious without ground.\n",
            "  TRANIOLY. Sir Soldier that I complest I know. He biding\n",
            "no you for kep and the mother\n",
            "34827/34827 [==============================] - 195s 6ms/step - loss: 1.4262\n",
            "Epoch 12/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.4170\n",
            "Epoch 13/50\n",
            "34827/34827 [==============================] - 184s 5ms/step - loss: 1.4135\n",
            "Epoch 14/50\n",
            "34827/34827 [==============================] - 185s 5ms/step - loss: 1.6917\n",
            "Epoch 15/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 2.0594\n",
            "Epoch 16/50\n",
            "34824/34827 [============================>.] - ETA: 0s - loss: 1.5918\n",
            "----- Generating text after Epoch: 15\n",
            "----- Generating with seed: \"lmes, or put to sea,\n",
            "Or tell of Babes br\"\n",
            "lmes, or put to sea,\n",
            "Or tell of Babes briees patchances, which I live the ill.\n",
            "  That is nove knights, grow mine eye, told upon, butbleing and twop'd, eliege how apppisiafifis cans!\n",
            "  ROSALINCE. So sure not every to-derire.\n",
            "  TADY. Nay, ye seree. Thou speaking says full,\n",
            "    Then gom in Beetras them then, with his raillow conscued,\n",
            "Stafulo cousced-day of the humbey imids\n",
            "    Of your beautio supplayed ome stran\n",
            "figuring creaps to not alo\n",
            "34827/34827 [==============================] - 196s 6ms/step - loss: 1.5918\n",
            "Epoch 17/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.4627\n",
            "Epoch 18/50\n",
            "34827/34827 [==============================] - 183s 5ms/step - loss: 1.4112\n",
            "Epoch 19/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3999\n",
            "Epoch 20/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3935\n",
            "Epoch 21/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 1.3882\n",
            "----- Generating text after Epoch: 20\n",
            "----- Generating with seed: \"common men.\n",
            "\n",
            "KING HENRY.\n",
            "This note doth \"\n",
            "common men.\n",
            "\n",
            "KING HENRY.\n",
            "This note doth standes it your behord,\n",
            "I told bent of you lies eye errasts tormers ingell?\n",
            "\n",
            " MENENAUS.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "That’s most certain.\n",
            "\n",
            "FIRST POMANDO.\n",
            "Our one them.\n",
            "\n",
            "DOCTOR.\n",
            "My, what, yet that, you told, a parah!\n",
            "\n",
            "            Eneen\n",
            "  ANGELDOUS OS] Grace of they will scorn; for sill stip; in Vief,\n",
            "    Where his excupser service, grow on state,\n",
            "    Tad to liegnisRy and one farber'd to deft.\n",
            "  SUFFOLK. To in home is\n",
            "    King,\n",
            "34827/34827 [==============================] - 195s 6ms/step - loss: 1.3882\n",
            "Epoch 22/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3836\n",
            "Epoch 23/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3800\n",
            "Epoch 24/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3767\n",
            "Epoch 25/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3738\n",
            "Epoch 26/50\n",
            "34820/34827 [============================>.] - ETA: 0s - loss: 1.3711\n",
            "----- Generating text after Epoch: 25\n",
            "----- Generating with seed: \" shore to shore, and left me breath\n",
            "Noth\"\n",
            " shore to shore, and left me breath\n",
            "Noth make my sake, this soad the assoance stain by\n",
            "From desire to have mistatim! I do how five a gracifish art yet?\n",
            "    What kissuaus we that I give me tainted.\n",
            "  MURDEMAR. They inseep to divine?\n",
            "  PHOTER.\n",
            "               The Capultudy, MAERON\n",
            "    Clarence you can from the worfine, that they\n",
            "ware my further estriction to hear recovied\n",
            "    or calleth best starding leds. ansterers was o't, and what be gl\n",
            "34827/34827 [==============================] - 193s 6ms/step - loss: 1.3711\n",
            "Epoch 27/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3690\n",
            "Epoch 28/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3662\n",
            "Epoch 29/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3650\n",
            "Epoch 30/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3630\n",
            "Epoch 31/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 1.3613\n",
            "----- Generating text after Epoch: 30\n",
            "----- Generating with seed: \"ntaged, and\n",
            "    the corrupt deputy scale\"\n",
            "ntaged, and\n",
            "    the corrupt deputy scale.\n",
            "  MENENIUS. What's than you tell him clother, sir, Troes never for\n",
            "    rome all, for that a part a tending th'\n",
            "    tender we say ave dance;\n",
            "    With blush burn. Why, quoth the world sor fools to\n",
            "    well; nor that Lucius here can be not such and touch\n",
            "More seased ain makes a se.'NW thee with less part out, boose son that King of thy\n",
            "shall all tonight her?\n",
            "She I am so: I do! a less thy than the b\n",
            "34827/34827 [==============================] - 195s 6ms/step - loss: 1.3613\n",
            "Epoch 32/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3596\n",
            "Epoch 33/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3580\n",
            "Epoch 34/50\n",
            "34827/34827 [==============================] - 178s 5ms/step - loss: 1.3568\n",
            "Epoch 35/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3557\n",
            "Epoch 36/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 1.3543\n",
            "----- Generating text after Epoch: 35\n",
            "----- Generating with seed: \"nk, Timon.\n",
            "  SECOND LORD. Joy had the li\"\n",
            "nk, Timon.\n",
            "  SECOND LORD. Joy had the life in head?\n",
            "  SPEED. Gave, my Hign alonion; I'll bid not?\n",
            "  CATES. Farthow, whom Ay, huge them, in his ribblipfs so thine\n",
            "    The grassing ageith's too blood a pride man\n",
            "    My groanst howeden. No, I am tocking'd.\n",
            "  HAST. Exeunt LADY\n",
            "\n",
            "\n",
            "\n",
            "ACT IV\n",
            "\n",
            "SCENE III. corners\n",
            "\n",
            " Enter Fenelate would that faith having. I'll you alter, and right!\n",
            "  KATHARINE. Thou art but the world should sweet far ittle forefore\n",
            "34827/34827 [==============================] - 194s 6ms/step - loss: 1.3543\n",
            "Epoch 37/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3527\n",
            "Epoch 38/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3522\n",
            "Epoch 39/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3509\n",
            "Epoch 40/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3498\n",
            "Epoch 41/50\n",
            "34826/34827 [============================>.] - ETA: 0s - loss: 1.3489\n",
            "----- Generating text after Epoch: 40\n",
            "----- Generating with seed: \"O. Well; come to me to-morrow.\n",
            "  LUCIO. \"\n",
            "O. Well; come to me to-morrow.\n",
            "  LUCIO. Will, give't, you will?\n",
            "  TITUS. Melany!\n",
            "  JULIA. My lord, is Somerset, sleep where I send it.\n",
            "\n",
            "AEDMONDRANE.\n",
            "Ay, yes all his whills to be a tende,\n",
            "And I heard yeeds of the vow.\n",
            "\n",
            " Enter Duke of O crick, Clower.\n",
            "\n",
            "MENELAUS.\n",
            "[Cry office; for how I do foul under me enough ear;\n",
            "    There's not he meet their hence and thought upon your shadow;\n",
            "    As thy comfortsious my tolmeth and require\n",
            "    And there \n",
            "34827/34827 [==============================] - 195s 6ms/step - loss: 1.3490\n",
            "Epoch 42/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3478\n",
            "Epoch 43/50\n",
            "34827/34827 [==============================] - 181s 5ms/step - loss: 1.3471\n",
            "Epoch 44/50\n",
            "34827/34827 [==============================] - 182s 5ms/step - loss: 1.3460\n",
            "Epoch 45/50\n",
            "34827/34827 [==============================] - 183s 5ms/step - loss: 1.3459\n",
            "Epoch 46/50\n",
            "34818/34827 [============================>.] - ETA: 0s - loss: 1.3446\n",
            "----- Generating text after Epoch: 45\n",
            "----- Generating with seed: \"ter\n",
            "Than in the note of judgement; and w\"\n",
            "ter\n",
            "Than in the note of judgement; and which showers lose walked\n",
            "Cyrculvous brecine for feasting had the England?\n",
            "  POMTEYNE. O, coming you will her.                  Exeug SELATAMO wracture hath Hontuse\n",
            "ULoo field to France.\n",
            "\n",
            "PERICLES.\n",
            "If Geem I a fitre being all.\n",
            "\n",
            "LUCENTIO.\n",
            "I might be their engions of from his\n",
            "Gract to diathregui’d to strange. Have I laid me\n",
            "That intermiging sac those poor and tears. Come\n",
            "    Whose thing wan before, a\n",
            "34827/34827 [==============================] - 193s 6ms/step - loss: 1.3446\n",
            "Epoch 47/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3437\n",
            "Epoch 48/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3404\n",
            "Epoch 49/50\n",
            "34827/34827 [==============================] - 178s 5ms/step - loss: 1.3426\n",
            "Epoch 50/50\n",
            "34827/34827 [==============================] - 180s 5ms/step - loss: 1.3422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c4a1cb5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ccM_9DbKst",
        "colab_type": "text"
      },
      "source": [
        "### Generate a single prediction at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUltwZsbfb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separating some code from the on_epoch_end function earlier\n",
        "def predict(prediction_model, length=400):\n",
        "  \"\"\" \n",
        "  returns a random prediction from the model\n",
        "  \n",
        "  param length: the number of characters to generate\n",
        "  \"\"\"\n",
        "\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  seed = text[start_index: start_index + maxlen]\n",
        "  generated = seed\n",
        "  \n",
        "  print('----- Generating with seed: \"' + seed + '\"\\n')\n",
        "  \n",
        "  for i in range(length):\n",
        "    # encode the seed for the model\n",
        "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(seed):\n",
        "      x_pred[0, t, char_int[char]] = 1\n",
        "    \n",
        "    # make a prediction\n",
        "    preds = prediction_model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "    # convert back from index to character\n",
        "    next_index = sample(preds)\n",
        "    next_char = int_char[next_index]\n",
        "    \n",
        "    # shift seed for the next prediction\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "    # save the generated character to the output\n",
        "    generated += next_char\n",
        "\n",
        "  return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZb01FxlqNrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "      \n",
        "generated = ''\n",
        "\n",
        "sentence = text[start_index: start_index + maxlen]\n",
        "generated += sentence\n",
        "\n",
        "print('----- Generating with seed: \"' + sentence + '\"')\n",
        "sys.stdout.write(generated)\n",
        "\n",
        "for i in range(400):\n",
        "  x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "  for t, char in enumerate(sentence):\n",
        "    x_pred[0, t, char_int[char]] = 1\n",
        "      \n",
        "  preds = model.predict(x_pred, verbose=0)[0]\n",
        "  next_index = sample(preds)\n",
        "  next_char = int_char[next_index]\n",
        "  \n",
        "  sentence = sentence[1:] + next_char\n",
        "  \n",
        "  sys.stdout.write(next_char)\n",
        "  sys.stdout.flush()\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCCXj1M_emuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "2c86e293-c9f1-4b24-c3c4-2ecca3ed01c7"
      },
      "source": [
        "print(predict(model, length=500))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Generating with seed: \"oud.\n",
            "\n",
            "DIOMEDES.\n",
            "Or covetous of praise.\n",
            "\n",
            "\"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "oud.\n",
            "\n",
            "DIOMEDES.\n",
            "Or covetous of praise.\n",
            "\n",
            "LEONTES.\n",
            "Yes, death, late.\n",
            "\n",
            "CLOWN.\n",
            "Is this capbray.\n",
            "\n",
            "GOW, Godsan belieun\n",
            "\n",
            "Abate stareter’ told of uniters of me that he is\n",
            "      the world to had this no one. Let station is nothing.\n",
            "\n",
            "           Enter PAGE. Not forgen your part you gone?\n",
            "  OLIVER. Look I buke were penarition\n",
            "    To deny lustiess and will of schoducude\n",
            "Do us that answer and that must your form wound late,\n",
            "Make so much hands pains to peacent vown appare,\n",
            "In shills and to ridest nor thought.\n",
            "You do come to yet. Making Claudio thy\n",
            "f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qVlc0g_bfq1",
        "colab_type": "text"
      },
      "source": [
        "### Save and download model so I can use it again without re-training\n",
        "\n",
        "https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeu6Yceqbq8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fde979fe-1319-4c6c-c0f0-bb1cb95bdb35"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XOmCsmgdsVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model to a file\n",
        "model.save(\"shakespear_character_model.h5\")\n",
        "\n",
        "# Download from colab to my local machine, for later\n",
        "from google.colab import files\n",
        "files.download('shakespear_character_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89coUZ76blnC",
        "colab_type": "text"
      },
      "source": [
        "### How to load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-62eY7tbqm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# load model\n",
        "loaded_model = load_model('shakespear_character_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA9A8v3CfBo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2e5ada0d-266b-4ebb-8b6d-ca9298d91f93"
      },
      "source": [
        "# demonstrate that the loaded model can make the same predictions\n",
        "predict(loaded_model, length=500)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Generating with seed: \", and it no more merits\n",
            "The tread of a m\"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\", and it no more merits\\nThe tread of a men in great rest cast out,\\n    As joyled them young his tomons to us,\\n  RAILD anoolonalios, true.\\n\\n Enter Beatry and Duke the neglorien:—to me, look you ediour vain to\\nSo might brother.\\n\\n [_Exit._]\\n\\n [_Exeupings._]\\n\\nPAROLLES.\\nA villain is approal.\\nI becom his fleet is peace up.       Olle here have even to a spack known ungues charge,\\nA toil ear alonas. What arse sir, my kinent palace:\\n More was bound his bound, crysun'd of penest.\\n    Hearth of Romen to forsack'd and law not in\\n    Is this trus\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zqXYnAjNuj",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goal: tokenize each word and train a model with words instead of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EOMttvjVMb",
        "colab_type": "text"
      },
      "source": [
        "### Spacy tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP8FSLzMjYPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "def get_tokens(doc_array):\n",
        "    tokens = []\n",
        "    for doc in tokenizer.pipe(doc_array, batch_size=500):\n",
        "        tokens.append([token.text for token in doc])\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqCSLuvijuK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# break the text up into chunks of words\n",
        "\n",
        "chunk_length = 20  # words in each chunk\n",
        "\n",
        "# get an array of all words\n",
        "temp = get_tokens(text)\n",
        "\n",
        "w = 0\n",
        "words = []\n",
        "while word < len(temp):\n",
        "  for i in range(chunk_length):\n",
        "    words += temp[w]\n",
        "    w += 1\n",
        "\n",
        "print(f\"Text separated into {len(words)} chunks of {chunk_length} words each\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJbSS3BGmXw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8jLa2tXnXmh",
        "colab_type": "text"
      },
      "source": [
        "### Using gensim's Dictionary structure to encode those words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrQS24lPmwpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import corpora"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5gFiv0ujYsF",
        "colab_type": "text"
      },
      "source": [
        "### Create and fit a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238RpFFvjdBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqKkWdnyjdgL",
        "colab_type": "text"
      },
      "source": [
        "### Making predictions with this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfEKInjEmx5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRI-BXU_jkHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-flcDqbJjgx5",
        "colab_type": "text"
      },
      "source": [
        "### Saving and loading for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfh5_qdijjtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}