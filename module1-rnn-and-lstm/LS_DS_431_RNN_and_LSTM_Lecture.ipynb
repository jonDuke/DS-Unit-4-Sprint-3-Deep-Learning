{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LS_DS_441_RNN_and_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ldr0HZ193GKb"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 1*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4LQfEvdTjVK",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trd_GTKETjVK",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "# Neural Networks for Sequences (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgTXYsfiTjVM"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GrHlaXvFTjVN"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "outputId": "c232c5fb-87f2-48a1-e636-f7bd9d52f417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHu5c9Z7TjVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98819710-1a44-4130-fd68-f920603e9633"
      },
      "source": [
        "print('length: ', len(x_train[0]))\n",
        "x_train[0][:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length:  218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iljc6HxTjVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9735cb72-b85f-4a2b-df08-ca019f68ef5b"
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad Sequences (samples x time)\n",
            "x_train shape:  (25000, 80)\n",
            "x_test shape:  (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLoy5y-CTjVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d02ea361-866c-45e1-bca6-5d8ec47fcbf2"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
              "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
              "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
              "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
              "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
              "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
              "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
              "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
              "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxe6rCBzTjVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "bf831305-2589-452f-d2bd-cff230aadd2f"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5uVoIOSTjVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "aa7782a0-5015-4890-c21a-da3644004963"
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test,y_test))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 375s 479ms/step - loss: 0.4347 - accuracy: 0.7950 - val_loss: 0.3893 - val_accuracy: 0.8310\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 373s 477ms/step - loss: 0.2562 - accuracy: 0.8990 - val_loss: 0.3871 - val_accuracy: 0.8252\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 374s 478ms/step - loss: 0.1636 - accuracy: 0.9384 - val_loss: 0.4419 - val_accuracy: 0.8248\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 373s 477ms/step - loss: 0.1090 - accuracy: 0.9606 - val_loss: 0.5680 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 373s 477ms/step - loss: 0.0719 - accuracy: 0.9756 - val_loss: 0.6144 - val_accuracy: 0.8212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP8D_5OTTjVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5181c72d-cf12-4dd1-9042-bfec5c511464"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yQmfhkTjVh",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "# LSTM Text generation with Keras (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UUxY6ZnLTjVi"
      },
      "source": [
        "## Overview\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x1zZEwvTjVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul4ktsbdTjVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bcd3f869-b6f8-4114-c2d1-da4c07927e81"
      },
      "source": [
        "data = pd.read_json('https://raw.githubusercontent.com/jonDuke/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/wp_articles.json')\n",
        "\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When President Trump announced his decision to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>Like an aging rock star, the president is now ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article\n",
              "0    Contributing columnist\\n\\nThe House is on fire...\n",
              "1    When President Trump announced his decision to...\n",
              "10   Russian President Vladimir Putin speaks at a s...\n",
              "100  “The Queen’s Speech” is designed to acknowledg...\n",
              "101  Like an aging rock star, the president is now ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BioNDbYMn59d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to a plain list\n",
        "data = data['article'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN7dLLJrTjVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8b6baaa5-3c42-4b50-fd17-51fbfaee0a3c"
      },
      "source": [
        "data[-1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The threat to hard-won women’s rights in Rojava is receiving little coverage in the context of Turkey’s military campaign, but women there say Turkish aggression could wipe out these reforms and perhaps herald a return to the misogyny and sexual violence of militant Islamism. There is widespread concern about the possible escape of ISIS prisoners held by Kurdish forces, and on Sunday, it was reported that at least 750 people suspected of affiliation with ISIS fled a secure displacement camp in the chaos caused by Turkish shelling. In addition, several dozen “high-value” ISIS prisoners were reportedly left behind by U.S. troops when they retreated, the New York Times reported, and ISIS has already claimed at least two attacks in the area since the invasion started, including a car bombing that killed three people.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn5E9CluTjVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96jS1DSbTjVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f31e19e4-e2ac-4e95-d68e-79ec27dc272c"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "201WxR7tTjV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "314c628d-3963-4d38-bcf2-62877c09c525"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  178374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5x0ZIgTjV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "a697b2e7-cff2-4d85-f521-fb1b6d6ad559"
      },
      "source": [
        "sequences[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[112,\n",
              " 68,\n",
              " 48,\n",
              " 84,\n",
              " 51,\n",
              " 33,\n",
              " 3,\n",
              " 35,\n",
              " 84,\n",
              " 33,\n",
              " 48,\n",
              " 93,\n",
              " 27,\n",
              " 85,\n",
              " 68,\n",
              " 43,\n",
              " 35,\n",
              " 114,\n",
              " 48,\n",
              " 33,\n",
              " 0,\n",
              " 84,\n",
              " 14,\n",
              " 14,\n",
              " 41,\n",
              " 11,\n",
              " 24,\n",
              " 27,\n",
              " 15,\n",
              " 68,\n",
              " 35,\n",
              " 0,\n",
              " 24,\n",
              " 27,\n",
              " 33,\n",
              " 0,\n",
              " 27,\n",
              " 68,\n",
              " 48,\n",
              " 27]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyjZabn0TjV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyx0VXD3TjV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae97f5a3-b632-44b3-dfa9-8062f5918322"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 40, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pbx59H5TjV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99e3b1cf-ff3f-4fe2-9b93-7014b14ad258"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178374, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrwvK0ZzTjV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGgYh21MTjWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRpCm1MxTjWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VdVqg-KTjWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "baab95c8-5a4b-434d-e6d9-e42285691c6b"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5575/5575 [==============================] - ETA: 0s - loss: 2.5798\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"sented as a G.I. Joe action figure. Howe\"\n",
            "sented as a G.I. Joe action figure. Howe Pred. Thels ol hating dericean; Tr. Spaale anctimlson nfond A in Civitles, the plaller\n",
            "\n",
            "1I Rre shaory whe harnd a and astlek tive Tselsed dandere forciafd\n",
            "\n",
            "en Arrecrictan, ir suroaents ox s“in iv.\n",
            "\n",
            ":.\n",
            "\n",
            "“the urisad ánoudd th wark sotyr aor disulyer Alenag ouctem, Hrais, in Copstitichang blarinln. Ansinateductar forkrad ald prosuirot orkrsy RNSs pertisitl’umind ovtuine the to (w theres and trovk a \n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 2.5798\n",
            "Epoch 2/10\n",
            "5574/5575 [============================>.] - ETA: 0s - loss: 2.2327\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \" the escape of some Islamic State prison\"\n",
            " the escape of some Islamic State prisone thion wroke. At shates wutitent” expe in ere roms be that let modating wan’, 20 The cal-bo fonna as to butenct.\n",
            "\n",
            "L Puure. Al\n",
            "\n",
            "TA Mrinas the promenay, ant a ratire and nam..\n",
            "\n",
            "D Ramp’s spaluile in unted sagked you sad, is a juspbinf,” “11 in eraite 1his sours and F5eche sait prmoko yoe a pondayebin ow ex. Give as was graymers coxprolit ellinf in hint unviry of $Pace SSursSing-Eosould Nos Concive k\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 2.2327\n",
            "Epoch 3/10\n",
            "5572/5575 [============================>.] - ETA: 0s - loss: 2.0969\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"dvertising, and social media plug-ins) t\"\n",
            "dvertising, and social media plug-ins) that to a. Hienod bave trar, the afthictsor frestion erfire praply,” pucter officiling gerers pacints be sbligen damanenting the simering the but be darss ig nemuris it with can, the sand a loce, and revedtivedy. “The Unale was semister: Trureden Wiafe of s. Wastinghs — bus you “Ser (iglale Vallo: Iwhing a camery, sumpssmonter gho 4@irvic57 viters treen gitterar Summary of’t grough with gummarize h\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 2.0971\n",
            "Epoch 4/10\n",
            "5571/5575 [============================>.] - ETA: 0s - loss: 2.0049\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \" pressed Ukrainian President Volodymyr Z\"\n",
            " pressed Ukrainian President Volodymyr Z’d nat gamy, fom Thm ADss frow, pransiative has furate More. 1amrarte and riloghe, lewner frainels. Whink.”\n",
            "\n",
            "In’t mollerw that the cost, an wont pather toxter Fyrew. The “Rearisal some to not your who dadnon tho ywurh arcaid on The Firipial,” Senter at of the proCownis port desoonty, whilices to cresuly prays. Re W.S8 in the turmently. Oherstin of that’s somp resting’s billous and Yor Worch,””\n",
            "\n",
            "AD\n",
            "5575/5575 [==============================] - 83s 15ms/step - loss: 2.0052\n",
            "Epoch 5/10\n",
            "5571/5575 [============================>.] - ETA: 0s - loss: 1.9322\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"t to do. Many designs fall short of reso\"\n",
            "t to do. Many designs fall short of resorder mantentork, and 10 I fatouncwernnes, shrownt her dight thoss op viencipe, the torst thes un the dequert in a lone the Werkers, in it it Yor teges and Ressiations afthingly the ton firenteal Upraicaral Premilordonk., She 2013\n",
            "\n",
            "At NFEC spetia of globfiriged to canned begaijly telomdartod to and the age thim Rema a firesce is the coller bean to wech mulicenation ince thes’m orm sundermation it l\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 1.9320\n",
            "Epoch 6/10\n",
            "5571/5575 [============================>.] - ETA: 0s - loss: 1.8726\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"time. Director Jonathan Demme captured a\"\n",
            "time. Director Jonathan Demme captured an upters didma the bread” Eher RLSA fack but its rawnt ingrewded I dooded they.\n",
            "\n",
            "Servebormer that mize 0s her uphors diff.\n",
            "\n",
            "“I wreebst to are at-5 fin are doy to Ch5 Underpusing som Ocluad Unatery after placeman chiff bour not featch stresk’t bean juel off that the inforepwing for who implated that the “Pheiz to dist take takd sance now requictions pinaion, for not a laited dy the Syola” Mochdand \n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 1.8725\n",
            "Epoch 7/10\n",
            "5573/5575 [============================>.] - ETA: 0s - loss: 1.8220\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"a and the general public, the chief just\"\n",
            "a and the general public, the chief just, release Jof. Then repronne, sumges.\"\n",
            "\n",
            "Mazker the Omaran’s minities on Oneribe you ade of invicicite strom!” [Lup a proficion Sexiame Westivers canculing Afala Stony Trump saig. “Sthe Hindres, to limerngh a reiding Uprembic 201994 on tack, you repaaring they worher uirs, to Ceveni amoun frustive a charnee useration. The Nussign Son and morion have emby are equectiolt. “Thum mishifile of a dequest\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 1.8219\n",
            "Epoch 8/10\n",
            "5575/5575 [==============================] - ETA: 0s - loss: 1.7784\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"ny use of the Services, unless otherwise\"\n",
            "ny use of the Services, unless otherwise theer, feams wroth inglonace.\n",
            "Bo.., and thearseing ab the “mike Lisdime. Wrate anA\n",
            "\n",
            "Shoucl hoshington Whill CrumbCrict Dair,” churge nighal halvy of sicelizals monthiff.\n",
            "\n",
            "In adveined has pataed to goild him to year-outter with aseel ubbeasemmers wnowh a $A grock” he’s have cines tiget site thist has forgetion that factders on Trump ackeas tomn Cailerce Proind their wained toke will reasidely inso\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 1.7784\n",
            "Epoch 9/10\n",
            "5573/5575 [============================>.] - ETA: 0s - loss: 1.7399\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"w’s growing influence in the Middle East\"\n",
            "w’s growing influence in the Middle Easty Prode if in akd may furred Alainty. Read and she giver for the tweer your were, and vitelized dispecard, geteral whet spates works and othbated advines arraal on Tharm-hatse Wascly has beingrous, and definss your ellsy, you said courded have it.\n",
            "\n",
            "[Tthe as are kings requariced to her experitions a regrese tham thry sonal Ottermedy Trump wa pealse the groupy strimy floome by the time has player de\n",
            "5575/5575 [==============================] - 82s 15ms/step - loss: 1.7398\n",
            "Epoch 10/10\n",
            "5572/5575 [============================>.] - ETA: 0s - loss: 1.7059\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"anel, told the attorney.\n",
            "\n",
            "The cases coul\"\n",
            "anel, told the attorney.\n",
            "\n",
            "The cases could ret her ary a have their only can of the timesed to protigy.\n",
            "\n",
            "AP make Tinced mo Pont that ambant ex of the Uniter and Highbars, Just you Karging.\n",
            "\n",
            "As Pamniss a time it langer, op and struct of For S&p part of survicosing to cloon said their o-nouthichts host Vacy and Jon to Washow Tikes and, it Jost and The NFAgalam at Kanday Serflect sost that vigw the governge oromices ramed the higher said hi\n",
            "5575/5575 [==============================] - 83s 15ms/step - loss: 1.7059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98b59f7c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1pIsA1tTjWI",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use a Keras LSTM to generate text on today's assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfOm2L3YTjWI",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "    * Sequence Problems:\n",
        "        - Time Series (like Stock Prices, Weather, etc.)\n",
        "        - Text Classification\n",
        "        - Text Generation\n",
        "        - And many more! :D\n",
        "    * LSTMs are generally preferred over RNNs for most problems\n",
        "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
        "    * Keras has LSTMs/RNN layer types implemented nicely\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
        "    * Shape of input data is very important\n",
        "    * Can take a while to train\n",
        "    * You can use it to write movie scripts. :P "
      ]
    }
  ]
}